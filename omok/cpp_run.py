# cpp_run.py
# =============================================================================
# AlphaZero Omok (Gomoku) - C++ MCTS ì—”ì§„ + Python GPU ì¶”ë¡  í†µí•© êµ¬í˜„
# =============================================================================
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import ray
import time
import os
import asyncio
from collections import deque
import random
import matplotlib.pyplot as plt

# C++ë¡œ ë¹Œë“œí•œ ëª¨ë“ˆ ì„í¬íŠ¸
import mcts_core 

# =============================================================================
# [1] ì„¤ì •
# =============================================================================
BOARD_SIZE = 15
NUM_RES_BLOCKS = 5      
NUM_CHANNELS = 64       
BATCH_SIZE = 512        
INFERENCE_BATCH_SIZE = 256  # GPU í™œìš©ì„ ìœ„í•´ ë°°ì¹˜ í‚¤ì›€
LR = 0.001
BUFFER_SIZE = 200000    
NUM_ACTORS = 20         # 5950X ì½”ì–´ ìˆ˜ ê³ ë ¤ (ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹­ ë°©ì§€)
SAVE_INTERVAL = 20000

TRAINER_DEVICE = torch.device("cuda:0") 
INFERENCE_DEVICE = torch.device("cuda:0") 

# =============================================================================
# [2] ì‹ ê²½ë§ (ë³€ê²½ ì—†ìŒ)
# =============================================================================
class ResBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(channels)
    def forward(self, x):
        res = x
        x = F.relu(self.bn1(self.conv1(x)))
        x = self.bn2(self.conv2(x))
        return F.relu(x + res)

class AlphaZeroNet(nn.Module):
    def __init__(self):
        super().__init__()
        self.start_conv = nn.Conv2d(3, NUM_CHANNELS, 3, padding=1)
        self.bn_start = nn.BatchNorm2d(NUM_CHANNELS)
        self.backbone = nn.Sequential(*[ResBlock(NUM_CHANNELS) for _ in range(NUM_RES_BLOCKS)])
        self.policy_head = nn.Sequential(
            nn.Conv2d(NUM_CHANNELS, 2, 1), nn.BatchNorm2d(2), nn.ReLU(),
            nn.Flatten(), nn.Linear(2 * BOARD_SIZE * BOARD_SIZE, BOARD_SIZE * BOARD_SIZE)
        )
        self.value_head = nn.Sequential(
            nn.Conv2d(NUM_CHANNELS, 1, 1), nn.BatchNorm2d(1), nn.ReLU(),
            nn.Flatten(), nn.Linear(BOARD_SIZE * BOARD_SIZE, 64), nn.ReLU(),
            nn.Linear(64, 1), nn.Tanh()
        )
    def forward(self, x):
        x = F.relu(self.bn_start(self.start_conv(x)))
        x = self.backbone(x)
        policy = F.log_softmax(self.policy_head(x), dim=1)
        value = self.value_head(x)
        return policy, value

# =============================================================================
# [3] Inference Server
# =============================================================================

@ray.remote(num_gpus=0.4) # GPU ë©”ëª¨ë¦¬ ì ìœ ìœ¨ ì¡°ì •
class InferenceServer:
    def __init__(self):
        self.model = AlphaZeroNet().to(INFERENCE_DEVICE)
        self.model.eval()
        self.queue = asyncio.Queue()
        self.loop = asyncio.get_event_loop()
        self.loop.create_task(self.run_batch_inference())

    def update_weights(self, weights):
        self.model.load_state_dict(weights)

    async def predict(self, state_numpy):
        # state_numpy shape: (N, 3, 15, 15) where N <= 8
        future = self.loop.create_future()
        await self.queue.put((state_numpy, future))
        return await future

    async def run_batch_inference(self):
        while True:
            batch_inputs = []
            futures = []
            
            # 1. ì²« ë²ˆì§¸ ìš”ì²­ ëŒ€ê¸°
            item = await self.queue.get()
            batch_inputs.append(item[0]) # item[0] is (N, 3, 15, 15)
            futures.append(item[1])
            
            # 2. ë°°ì¹˜ ëª¨ìœ¼ê¸° (ìµœëŒ€ 3ms ëŒ€ê¸°)
            deadline = self.loop.time() + 0.003
            
            # í˜„ì¬ê¹Œì§€ ëª¨ì¸ ì´ ìƒ˜í”Œ ìˆ˜ ê³„ì‚° (Max Batch Size ì´ˆê³¼ ë°©ì§€ìš©)
            current_batch_size = item[0].shape[0]
            
            while True:
                timeout = deadline - self.loop.time()
                if timeout <= 0: break
                if current_batch_size >= INFERENCE_BATCH_SIZE: break # ë°°ì¹˜ ê½‰ ì°¨ë©´ ì¤‘ë‹¨
                
                try:
                    item = await asyncio.wait_for(self.queue.get(), timeout=timeout)
                    batch_inputs.append(item[0])
                    futures.append(item[1])
                    current_batch_size += item[0].shape[0]
                except asyncio.TimeoutError:
                    break
            
            if batch_inputs:
                with torch.no_grad():
                    # [ìˆ˜ì • í•µì‹¬] np.stack ëŒ€ì‹  np.concatenate ì‚¬ìš©!
                    # List of (N, 3, 15, 15) -> (Total_N, 3, 15, 15)
                    states_np = np.concatenate(batch_inputs, axis=0)
                    
                    states = torch.tensor(states_np, dtype=torch.float32).to(INFERENCE_DEVICE)
                    
                    # ì¶”ë¡ 
                    pi_logits, values = self.model(states)
                    
                    probs = torch.exp(pi_logits).cpu().numpy()
                    vals = values.cpu().numpy()
                
                # 3. ê²°ê³¼ ë¶„ë°° (Slicing back to workers)
                cursor = 0
                for i, future in enumerate(futures):
                    if not future.done():
                        # ì´ ì›Œì»¤ê°€ ë³´ëƒˆë˜ ìƒ˜í”Œ ê°œìˆ˜ í™•ì¸
                        num_samples = batch_inputs[i].shape[0]
                        
                        # ê²°ê³¼ ë°°ì—´ì—ì„œ í•´ë‹¹ ë²”ìœ„ë§Œí¼ ì˜ë¼ì„œ ì „ë‹¬
                        p_batch = probs[cursor : cursor + num_samples]
                        v_batch = vals[cursor : cursor + num_samples]
                        
                        future.set_result((p_batch, v_batch))
                        cursor += num_samples
                        
# =============================================================================
# [4] Worker (C++ MCTS íƒ‘ì¬)
# =============================================================================
@ray.remote(num_cpus=1)
class DataWorker:
    def __init__(self, buffer_ref, inference_server):
        self.buffer_ref = buffer_ref
        self.inference_server = inference_server
        
        # [í•µì‹¬] í•œ ì›Œì»¤ê°€ ë™ì‹œì— ê´€ë¦¬í•  ê²Œì„ ê°œìˆ˜ (CPU ì½”ì–´ë‹¹ 1ê°œì§€ë§Œ, ë…¼ë¦¬ì  ê²Œì„ì€ ì—¬ëŸ¬ ê°œ)
        self.num_parallel_games = 8  
        self.mcts_envs = [mcts_core.MCTS() for _ in range(self.num_parallel_games)]
        
        # ê° ê²Œì„ë³„ ìƒíƒœ ì €ì¥ì†Œ
        self.histories = [[] for _ in range(self.num_parallel_games)]
        self.sim_counts = [0] * self.num_parallel_games
        self.step_counts = [0] * self.num_parallel_games
        
        # ì´ˆê¸°í™”
        for mcts in self.mcts_envs:
            mcts.reset()
            # ë…¸ì´ì¦ˆ ë¯¸ë¦¬ ì¶”ê°€ (ìƒˆ ê²Œì„ ì‹œì‘ ì‹œ)
            mcts.add_root_noise(0.3, 0.25)

    def get_equi_data(self, history):
        # (ê¸°ì¡´ê³¼ ë™ì¼)
        extend_data = []
        for state, pi, z in history:
            pi_board = pi.reshape(BOARD_SIZE, BOARD_SIZE)
            for i in [1, 2, 3, 4]:
                equi_state = np.array([np.rot90(s, k=i) for s in state])
                equi_pi = np.rot90(pi_board, k=i)
                extend_data.append([equi_state, equi_pi.flatten(), z])
                equi_state_flip = np.array([np.fliplr(s) for s in equi_state])
                equi_pi_flip = np.fliplr(equi_pi)
                extend_data.append([equi_state_flip, equi_pi_flip.flatten(), z])
        return extend_data

    def run(self):
        # ë¬´í•œ ë£¨í”„
        while True:
            # ==========================================
            # 1. ë°°ì¹˜ ìˆ˜ì§‘ (Batch Collection)
            # ==========================================
            states_to_infer = []
            indices_to_infer = []
            
            for i in range(self.num_parallel_games):
                TARGET_SIMS = 200 
                
                if self.sim_counts[i] < TARGET_SIMS:
                    leaf = self.mcts_envs[i].select_leaf()
                    
                    if leaf is not None:
                        # ì¶”ë¡  í•„ìš” -> ë°°ì¹˜ì— ì¶”ê°€
                        states_to_infer.append(leaf)
                        indices_to_infer.append(i)
                    else:
                        # [ì¤‘ìš” ìˆ˜ì •] ê²Œì„ì´ ëë‚œ ë…¸ë“œë„ ì‹œë®¬ë ˆì´ì…˜ 1íšŒë¡œ ì³ì•¼ í•¨!
                        # C++ ë‚´ë¶€ì ìœ¼ë¡œ ì´ë¯¸ backpropagate_valueë¥¼ ìˆ˜í–‰í–ˆìœ¼ë¯€ë¡œ ì¹´ìš´íŠ¸ë§Œ ì˜¬ë¦¬ë©´ ë¨
                        self.sim_counts[i] += 1
            
            # ==========================================
            # 2. GPU ì¶”ë¡  (Batch Inference)
            # ==========================================
            if states_to_infer:
                # ì—¬ëŸ¬ ê²Œì„ì˜ ìƒíƒœë¥¼ ë¬¶ì–´ì„œ í•œ ë²ˆì— ìš”ì²­ (Blocking)
                # ëŒ€ê¸°í•˜ëŠ” ë™ì•ˆ ë‹¤ë¥¸ ì—°ì‚°ì€ ì—†ì§€ë§Œ, í•œ ë²ˆ ê°”ë‹¤ ì˜¤ë©´ Nê°œ ê²Œì„ì´ ë™ì‹œì— ì§„í–‰ë¨
                states_np = np.stack(states_to_infer)
                policy_batch, value_batch = ray.get(self.inference_server.predict.remote(states_np))
                
                # ==========================================
                # 3. ê²°ê³¼ ë°˜ì˜ (Backpropagation)
                # ==========================================
                for idx, policy, value in zip(indices_to_infer, policy_batch, value_batch):
                    self.mcts_envs[idx].backpropagate(policy, value.item())
                    self.sim_counts[idx] += 1

            # ==========================================
            # 4. í–‰ë™ ì„ íƒ (Make Move)
            # ==========================================
            for i in range(self.num_parallel_games):
                TARGET_SIMS = 200
                
                # ì‹œë®¬ë ˆì´ì…˜ ëª©í‘œ ë‹¬ì„± ì‹œ ì‹¤ì œ ì°©ìˆ˜
                if self.sim_counts[i] >= TARGET_SIMS:
                    mcts = self.mcts_envs[i]
                    
                    # Temperature ì ìš©
                    temp = 1.0 if self.step_counts[i] < 30 else 0.1
                    state, pi = mcts.get_action_probs(temp)
                    
                    # ê¸°ë¡ ì €ì¥
                    current_player = mcts.get_current_player()
                    self.histories[i].append([state, pi, current_player])
                    
                    # í–‰ë™ ì„ íƒ
                    action = np.random.choice(len(pi), p=pi)
                    mcts.update_root_game(action)
                    self.step_counts[i] += 1
                    
                    # ì‹œë®¬ë ˆì´ì…˜ ì¹´ìš´íŠ¸ ë¦¬ì…‹ & ë£¨íŠ¸ ë…¸ì´ì¦ˆ ì¶”ê°€
                    self.sim_counts[i] = 0
                    mcts.add_root_noise(0.3, 0.25)
                    
                    # ê²Œì„ ì¢…ë£Œ í™•ì¸
                    is_game_over, winner = mcts.check_game_status()
                    
                    if is_game_over or self.step_counts[i] >= BOARD_SIZE * BOARD_SIZE:
                        # ë¬´ìŠ¹ë¶€ ì²˜ë¦¬ (Winnerê°€ 0ì´ë©´ Draw)
                        # Zê°’ ê³„ì‚°
                        processed_history = []
                        for h_state, h_pi, h_player in self.histories[i]:
                            if winner == 0:
                                z = 0.0
                            elif h_player == winner:
                                z = 1.0
                            else:
                                z = -1.0
                            processed_history.append([h_state, h_pi, z])
                        
                        # ë²„í¼ ì „ì†¡
                        augmented = self.get_equi_data(processed_history)
                        self.buffer_ref.add.remote(augmented)
                        
                        # í•´ë‹¹ ê²Œì„ ìŠ¬ë¡¯ ë¦¬ì…‹
                        mcts.reset()
                        mcts.add_root_noise(0.3, 0.25)
                        self.histories[i] = []
                        self.step_counts[i] = 0
                        self.sim_counts[i] = 0
                        
# =============================================================================
# [5] í•™ìŠµ ë£¨í”„ (Main)
# =============================================================================
@ray.remote
class ReplayBuffer:
    def __init__(self): self.buffer = deque(maxlen=BUFFER_SIZE)
    def add(self, history): self.buffer.extend(history)
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        s, pi, z = zip(*batch)
        return np.array(s), np.array(pi), np.array(z)
    def size(self): return len(self.buffer)

if __name__ == "__main__":
    if ray.is_initialized(): ray.shutdown()
    ray.init()
    
    print(f"ğŸš€ AlphaZero HYBRID (C++ Engine + Python GPU) Started!")
    print(f"run: watch -n 0.1 nvidia-smi (to check usage)")

    inference_server = InferenceServer.remote()
    buffer = ReplayBuffer.remote()
    
    # Worker ì‹œì‘
    workers = [DataWorker.remote(buffer, inference_server) for _ in range(NUM_ACTORS)]
    for w in workers: w.run.remote()
    
    model = AlphaZeroNet().to(TRAINER_DEVICE)
    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)
    step = 0
    loss_history = {'step': [], 'total': [], 'pi': [], 'v': []}
    
    print("Waiting for data generation...")
    last_train_size = 0
    
    while True:
        current_size = ray.get(buffer.size.remote())
        
        if current_size < BATCH_SIZE * 2:
            print(f"\rWarm-up... ({current_size}/{BATCH_SIZE*2})", end="")
            time.sleep(1)
            continue
            
        # ë°ì´í„° ìˆ˜ì§‘ ì†ë„ê°€ ë¹¨ë¼ì¡Œìœ¼ë¯€ë¡œ ë” ìì£¼ í•™ìŠµ
        s_batch, pi_batch, z_batch = ray.get(buffer.sample.remote(BATCH_SIZE))
        
        s_tensor = torch.tensor(s_batch, dtype=torch.float32).to(TRAINER_DEVICE)
        pi_tensor = torch.tensor(pi_batch, dtype=torch.float32).to(TRAINER_DEVICE)
        z_tensor = torch.tensor(z_batch, dtype=torch.float32).to(TRAINER_DEVICE).unsqueeze(1)
        
        pred_pi, pred_v = model(s_tensor)
        loss_pi = -torch.mean(torch.sum(pi_tensor * pred_pi, dim=1))
        loss_v = F.mse_loss(pred_v, z_tensor)
        total_loss = loss_pi + loss_v
        
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()
        
        step += 1
        
        if step % 10 == 0:
            print(f"[Step {step}] Loss: {total_loss.item():.4f} (Pi: {loss_pi.item():.4f}, V: {loss_v.item():.4f}) Buffer: {current_size}")
            loss_history['step'].append(step)
            loss_history['total'].append(total_loss.item())
            loss_history['pi'].append(loss_pi.item())
            loss_history['v'].append(loss_v.item())

        # ìµœì‹  ê°€ì¤‘ì¹˜ ì „ì†¡ (50 ìŠ¤í…ë§ˆë‹¤)
        if step % 50 == 0:
            cpu_weights = {k: v.cpu() for k, v in model.state_dict().items()}
            inference_server.update_weights.remote(cpu_weights)
            
        if step % SAVE_INTERVAL == 0:
            if not os.path.exists("models"): os.makedirs("models")
            torch.save(model.state_dict(), f"models/model_{step}.pth")
            # ê·¸ë˜í”„ ì €ì¥
            plt.figure(figsize=(12, 5))
            plt.subplot(1, 2, 1)
            plt.plot(loss_history['step'], loss_history['total'], label='Total')
            plt.plot(loss_history['step'], loss_history['pi'], label='Policy')
            plt.legend(); plt.grid(True)
            plt.subplot(1, 2, 2)
            plt.plot(loss_history['step'], loss_history['v'], label='Value', color='orange')
            plt.legend(); plt.grid(True)
            plt.savefig('training_loss.png')
            plt.close()
            print(f"ğŸ’¾ Model Saved & Graph Updated")